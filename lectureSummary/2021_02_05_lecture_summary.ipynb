{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ignored-invite",
   "metadata": {},
   "source": [
    "# Transformer 강의 정리\n",
    "\n",
    "# Attention Mechanism이란?\n",
    "\n",
    "- RNN 기반 seq2seq 모델의 문제\n",
    "    - 하나의 고정된 크기의 벡터에 모든 정보를 압축하려고 하니까 정보 손실이 발생\n",
    "    - 기울기 소실(Vanishing Gradient) 문제 (RNN의 고질적 문제)\n",
    "- Attention Mechanism: 디코더에서 출력 단어를 예측하는 매 시점마다 인코더에서의 전체 입력 문장을 다시 한 번 참고한다!\n",
    "    - $\\text{Attention}(Q, K, V)$\n",
    "        - Q(Query) : 질문 $Q$의 답이 무엇인가? (현재 hidden layer로 날리는 질문)\n",
    "        - K(Key) : 기존의 데이터의 key → $K$라는 key 에 대하여\n",
    "        - V(Value) : $K$라는 key에 해당하는 value $V$\n",
    "        - 즉, Attention의 값은 $K$라는 key에 대하여 $Q$라는 질문을 했을 때 나오는 답(연관성의 정도)을 value $V$와 곱한 값 → Attention Layer로 Encoder에서는 다른 단어와의 관계성 또한 encoding할 수 있음.\n",
    "\n",
    "# Transformer에서의 Attention\n",
    "\n",
    "- Multi-head Attention 사용\n",
    "\n",
    "![http://jalammar.github.io/images/t/Transformer_decoder.png](http://jalammar.github.io/images/t/Transformer_decoder.png)\n",
    "\n",
    "[http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/)\n",
    "\n",
    "## Self Attention\n",
    "\n",
    "- Encoder, Decoder Layer 내에 존재하는 attention layer\n",
    "    - 단, encoder에서는 모든 word에 대해 계산하지만 Decoder는 현재 레이어 이전 값만 계산 → Decoder는 Masked Attention 사용\n",
    "\n",
    "![https://wikidocs.net/images/page/31379/attention.PNG](https://wikidocs.net/images/page/31379/attention.PNG)\n",
    "\n",
    "[https://wikidocs.net/31379](https://wikidocs.net/31379)\n",
    "\n",
    "- 문장을 구성하는 각 단어 간의 상관관계를 구하는 layer\n",
    "    - Q: 현재 hidden layer에서 관장하는 단어\n",
    "    - K: 문장 전체 word의 key 값\n",
    "    - V: key에 해당하는 word value\n",
    "    - Q, K, V는 모두 같은 출처에서 나온 값\n",
    "- Self Attention은 현재 문장에서 특정 단어가 문장 전체 word의 key와 어떤 상관관계를 가지고 있는가? 라는 질문에 답할 수 있도록 함\n",
    "\n",
    "## Encoder-Decoder 간의 Attention\n",
    "\n",
    "- Encoder의 맥락에서 본 Decoder의 next Hidden Layer 값\n",
    "    - Q: Decoder의 이전 Layer에서 온 값\n",
    "    - K: Encoder에서 온 Key 값\n",
    "    - V: Encoder의 Key에 해당하는 Value\n",
    "\n",
    "## Multi-Head Attention\n",
    "\n",
    "![http://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png](http://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png)\n",
    "\n",
    "[http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/)\n",
    "\n",
    "- **그림 상에서는 Embedding Vector의 크기는 4, Q/K/V Vector의 크기는 3으로 나와있지만 이는 단순화된 이미지 → 실제 Embedding Vector의 크기는 512, Q/K/V Vector의 크기는 64이다.**\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}({QK^T \\over \\sqrt{d_k}})V$$\n",
    "\n",
    "$$\\text{MultiHead}(Q,K,V)=[head_1;\\cdots;head_h]W^O \\\\\n",
    "head_i=\\text{Attention}(RW^Q_i,RW^K_i,RW^V_i)$$\n",
    "\n",
    "- $R$ : 첫 번째 encoder layer에 들어가는 embedded input / 이전 encoder layer를 지난 결과값\n",
    "- $\\sqrt{d_k}$를 나누는 이유: more stable gradients\n",
    "- Why Multi-Head?\n",
    "    - multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n",
    "    - Convolution에서 Channel을 여러 개 주는 것과 비슷한 이유라고 이해함\n",
    "\n",
    "# Transformer Model의 구조\n",
    "\n",
    "![http://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png](http://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png)\n",
    "\n",
    "[http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/)\n",
    "\n",
    "## Positional Encoding\n",
    "\n",
    "- 각 단어의 위치 정보를 encoding 하는 Layer\n",
    "\n",
    "## Encoder Layer\n",
    "\n",
    "- Self-Attention\n",
    "- Add & Normalize (ResNet처럼 Residual)\n",
    "- Feed Foward\n",
    "    - 입력과 출력의 차원이 동일\n",
    "- Add & Normalize (Residual)\n",
    "\n",
    "### Decoder Layer\n",
    "\n",
    "- Self-Attention\n",
    "- Add & Normalize\n",
    "- Encoder-Decoder Attention\n",
    "- Add & Normalize\n",
    "- Feed Foward\n",
    "- Add & Normalize (Residual)\n",
    "\n",
    "### Linear & Softmax\n",
    "\n",
    "- 결과적으로 어떤 단어가 나와야 할 지를 결정\n",
    "\n",
    "# 코드\n",
    "\n",
    "- [동빈나님 Transformer 코드](https://colab.research.google.com/github/ndb796/Deep-Learning-Paper-Review-and-Practice/blob/master/code_practices/Attention_is_All_You_Need_Tutorial_(German_English).ipynb)\n",
    "\n",
    "## How Translation Works?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-overhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역(translation) 함수\n",
    "def translate_sentence(sentence, src_field, trg_field, model, device, max_len=50, logging=True):\n",
    "    model.eval() # 평가 모드\n",
    "\n",
    "    if isinstance(sentence, str):\n",
    "        nlp = spacy.load('de')\n",
    "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    # 처음에 <sos> 토큰, 마지막에 <eos> 토큰 붙이기\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "    if logging:\n",
    "        print(f\"전체 소스 토큰: {tokens}\")\n",
    "\n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "    if logging:\n",
    "        print(f\"소스 문장 인덱스: {src_indexes}\")\n",
    "\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "    # 소스 문장에 따른 마스크 생성\n",
    "    src_mask = model.make_src_mask(src_tensor)\n",
    "\n",
    "    # 인코더(endocer)에 소스 문장을 넣어 출력 값 구하기\n",
    "    with torch.no_grad():\n",
    "\t\t# Encoder의 경우 문장 전체 token을 바로 입력으로 넣어준다.\n",
    "        enc_src = model.encoder(src_tensor, src_mask)\n",
    "\n",
    "    # 처음에는 <sos> 토큰 하나만 가지고 있도록 하기\n",
    "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "\n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "        # 출력 문장에 따른 마스크 생성\n",
    "        trg_mask = model.make_trg_mask(trg_tensor)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        # 출력 문장에서 가장 마지막 단어만 사용\n",
    "        pred_token = output.argmax(2)[:,-1].item()\n",
    "        trg_indexes.append(pred_token) # 출력 문장에 더하기\n",
    "\n",
    "        # <eos>를 만나는 순간 끝\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "\n",
    "    # 각 출력 단어 인덱스를 실제 단어로 변환\n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "\n",
    "    # 첫 번째 <sos>는 제외하고 출력 문장 반환\n",
    "    return trg_tokens[1:], attention\n",
    "\n",
    "''' \n",
    "je suis etudient == I am a student\n",
    "\n",
    "<sos>\n",
    "-> <sos> I\n",
    "['I']\n",
    "\n",
    "-> <sos> I am\n",
    "['I', 'am']\n",
    "\n",
    "->\n",
    "...\n",
    "\n",
    "-> <sos> I am a student <eos>\n",
    "['I', 'am' 'a', 'student', '<eos>']\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-penetration",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- Transformer 모델은 Encoder-Decoder를 학습시키기 위한 모델\n",
    "    - 학습이 완료된 후 Translation을 사용할 때는 source string을 Encoder에 넣어 인코딩 시킨 다음 target string의 `<eos>` 에 도달할 때까지 decoder를 돌리는 형태\n",
    "\n",
    "# 참고 자료\n",
    "\n",
    "- [https://wikidocs.net/22893](https://wikidocs.net/22893)\n",
    "- [[동빈나] Attention Is All You Need 논문 리뷰](https://youtu.be/AA621UofTUA)\n",
    "- [Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "    - [한국어판](https://nlpinkorean.github.io/illustrated-transformer/)\n",
    "- [Attention? Attention!](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#multi-head-self-attention)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
