{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sapphire-commitment",
   "metadata": {},
   "source": [
    "# 2021/02/19 Lecture Summary\n",
    "\n",
    "## Transfer Learning\n",
    "![](https://d2l.ai/_images/finetune.svg)\n",
    "[이미지 출처: Dive Into Deep Learning](https://d2l.ai/chapter_computer-vision/fine-tuning.html)\n",
    "- Transfer Learning(전이학습): 하나의 소스 데이터셋에서 학습한 정보를 다른 타겟 데이터셋에 이용하는 학습 방식\n",
    "- Fine Tuning: 전이학습의 일종. 다른 분야의 풍부한 데이터를 바탕으로 한 좋은 성능의 모델에서 일부 계층을 재활용하여 모델을 구축하는 방법.\n",
    "\n",
    "\n",
    "## GPT-1\n",
    "### GPT-1의 구조\n",
    "![https://greeksharifa.github.io/public/img/2019-08-21-OpenAI%20GPT-1%20-%20Improving%20Language%20Understanding%20by%20Generative%20Pre-Training/01.png](https://greeksharifa.github.io/public/img/2019-08-21-OpenAI%20GPT-1%20-%20Improving%20Language%20Understanding%20by%20Generative%20Pre-Training/01.png)\n",
    "Improving Language Understanding by Generative Pre-Training\n",
    "- Transformer 모델 구조\n",
    "    - 12개의 layer\n",
    "    - Masked Multi Self Attention → Layer Norm → Feed Forward → Residual → Layer Norm\n",
    "        - Transformer의 Decoder에서 Encoder-Decoder Attention이 빠진 형태\n",
    "    \n",
    "- 하나의 모델로 다양한 NLP Task 해결 가능\n",
    "    - 입력하는 Data 값의 형태에 따라 원하는 종류의 Task를 해결할 수 있음\n",
    "    \n",
    "- GPT-1으로 풀 수 있는 NLP Task의 종류\n",
    "    - Classification (Sentiment Analysis): 감정 분류\n",
    "    - Entailment: Premise가 참이면 Hypothesis가 참인지 여부를 판단하는 Task\n",
    "    - Similarity: 두 문장의 유사도를 판단하는 Task\n",
    "    - Multiple Choice: 답이 여러개인 문제를 푸는 Task\n",
    "\n",
    "### GPT-1의 작동 방식\n",
    "\n",
    "1. Unsupervised pre-training\n",
    "    - Unsupervised Corpus에 대하여 Text Prediction을 수행하는 Transformer Decoder 형태의 모델을 학습시킨다.\n",
    "    - $\\Theta$ : Model Parameter, k: Context Window, u: token, $W_e$: token embedding matrix, $W_p$: position embedding matrix\n",
    "\n",
    "    $$L_1(\\mathcal{U})=\\sum_i\\text{log}P(u_i|u_{i-k},\\dots,u_{i-1};\\Theta) \\\\\n",
    "    h_0 = UW_e + W_p \\\\\n",
    "    h_l = \\text{transformer}(h_{l-1}) \\\\\n",
    "    P(u) = \\text{softmax}(h_nW_e^T)$$\n",
    "\n",
    "2. Supervised fine-tuning\n",
    "    - Adapt the parameters to the supervised target task\n",
    "    - 이미 Train 된 Transformer 레이어는 learning rate를 작게, supervised target task를 위해 붙이는 레이어의 learning rate는 크게 주어 기존에 학습한 데이터를 크게 해치지 않도록 설정\n",
    "    - 여기서 입력으로 주어지는 데이터의 형식은 Task-Specific\n",
    "    \n",
    "    $$\n",
    "    P(y|x^1, \\dots, x^m) = \\text{softmax}(h_l^mW_y) \\\\\n",
    "    L_2(\\mathcal{C}) = \\sum_{(x, y)}\\text{log}P(y|x^1, \\dots, x^m) \\\\\n",
    "    \\rightarrow L_3 = L_2(\\mathcal{C}) + \\lambda L_1(\\mathcal{C})\n",
    "    $$\n",
    "    \n",
    "    \n",
    "### 참고 자료\n",
    "\n",
    "- [Dive Into Deep Learning](https://d2l.ai/chapter_computer-vision/fine-tuning.html)\n",
    "- GPT-1 논문\n",
    "    - Paper: [https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)\n",
    "    - 논문리뷰 참고자료: [https://greeksharifa.github.io/nlp(natural language processing) / rnns/2019/08/21/OpenAI-GPT-1-Improving-Language-Understanding-by-Generative-Pre-Training/](https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2019/08/21/OpenAI-GPT-1-Improving-Language-Understanding-by-Generative-Pre-Training/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-heather",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
